{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_5b_Tfb7e4X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1a74d9b3-2752-4855-c612-00b97667abd6"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
        "import random\n",
        "import time\n",
        "from collections import deque\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=True, threshold=10000, linewidth=250)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq3rMK0A7qup",
        "colab_type": "text"
      },
      "source": [
        "**DQN Problem **\n",
        "\n",
        " Choose one of the hyperparameter types you want in the following list,\n",
        "\n",
        "\n",
        "1. self.batch_size = 64\n",
        "2. self.epsilon = 1.0 \n",
        "3. upate frequency of the target q network:\n",
        "```\n",
        "            if episode % 10 == 0: # an example.\n",
        "                self.update_target_q_weights()\n",
        "```\n",
        "4. the number of hidden units for the q network\n",
        "\n",
        "and train the agent over different values of the selected hyperparameter type.\n",
        "For example, if you selected the hyperparameter #2, then you might want to change the values over 0.1 ~ 1.0.\n",
        "In other words, if you train the agent on epsilon=0.1, 0.2,...,1.0, \n",
        "it might result in 10 different training instances and the corresponding learning curves (10 curves). \n",
        "You don't have to inspect too many hyperparameter values, 10 different values might be too much, \n",
        "but 3~7 values (e.g. [0.1, 0.3, 0.5, 0.7, 0.9]) might be sufficient.\n",
        "\n",
        "* Plot the learning curves over the different hyperparameter values \n",
        " (each learning curve represents episode rewards over iterations in a training instance).\n",
        "* Provide your own interpretation on what is the effect of the hyperparameter on the learning agent, as a comment in the code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r24pnG677rER",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer:\n",
        "\n",
        "    def __init__(self, memory_size=10000):\n",
        "        self.memory = deque(maxlen=memory_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = [], [], [], [], []\n",
        "        for _ in range(batch_size):\n",
        "            idx = np.random.randint(len(self.memory))\n",
        "            state, action, reward, next_state, done = self.memory[idx]\n",
        "            state_batch.append(state)\n",
        "            action_batch.append(action)\n",
        "            reward_batch.append(reward)\n",
        "            next_state_batch.append(next_state)\n",
        "            done_batch.append(done)\n",
        "        return np.array(state_batch), np.array(action_batch), np.array(reward_batch)[:, None], np.array(next_state_batch), np.array(done_batch)[:, None]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBmZfY6E7yr9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QNetwork(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, state_dim, num_actions):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.l1 = tf.keras.layers.Dense(64, activation='relu')\n",
        "        self.l2 = tf.keras.layers.Dense(64, activation='relu')\n",
        "        self.out = tf.keras.layers.Dense(num_actions)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        \"\"\"\n",
        "        :param state: [batch_size, state_dim] 크기의 Tensor\n",
        "        :return: [batch_size, num_actions] 크기의 Tensor\n",
        "        \"\"\"\n",
        "        state, = inputs\n",
        "        h = self.l1(state)\n",
        "        h = self.l2(h)\n",
        "        q_values = self.out(h)  # [batch_size, num_actions]\n",
        "        return q_values"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMLNjC6x74co",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQN:\n",
        "\n",
        "    def __init__(self, env, discount, epsilon=1.0):\n",
        "        self.env = env\n",
        "        self.state_dim = env.observation_space.shape[0]\n",
        "        self.num_actions = env.action_space.n\n",
        "        self.discount = discount\n",
        "\n",
        "        # Hyper parameters\n",
        "        self.batch_size = 64\n",
        "        self.epsilon = epsilon  # exploration rate\n",
        "        self.epsilon_min = 0.05\n",
        "        self.epsilon_decay = 0.997\n",
        "\n",
        "        # TF session / placeholders\n",
        "        self.sess = tf.keras.backend.get_session()\n",
        "        self.state_ph = tf.keras.layers.Input(self.state_dim, name='state', dtype=tf.float32)  # [batch_size, state_dim]\n",
        "        self.action_ph = tf.keras.layers.Input((), name='action', dtype=tf.int32)  # [batch_size]\n",
        "        self.reward_ph = tf.keras.layers.Input(1, name='reward', dtype=tf.float32)  # [batch_size, 1]\n",
        "        self.next_state_ph = tf.keras.layers.Input(self.state_dim, name='next_state', dtype=tf.float32)  # [batch_size, state_dim]\n",
        "        self.done_ph = tf.keras.layers.Input(1, name='done', dtype=tf.float32)  # [batch_size, 1]\n",
        "\n",
        "        # Q-Network / target Q-network\n",
        "        self.q = QNetwork(self.state_dim, self.num_actions)\n",
        "        self.q_target = QNetwork(self.state_dim, self.num_actions)\n",
        "\n",
        "        # Construct computation graph (loss function)\n",
        "        self.q_values = self.q([self.state_ph])  # [batch_size, num_actions]\n",
        "        action_mask = tf.one_hot(self.action_ph, self.num_actions) # [batch_size, num_actions]\n",
        "        q_sa = tf.reduce_sum(self.q_values * action_mask, axis=1, keepdims=True)  # [batch_size, 1]\n",
        "\n",
        "        next_q_values = self.q_target([self.next_state_ph])  # [batch_size, num_actions]\n",
        "        next_q = tf.stop_gradient(tf.reduce_max(next_q_values, axis=1, keepdims=True))  # [batch_size, 1]\n",
        "        y = self.reward_ph + self.discount * (1 - self.done_ph) * next_q\n",
        "\n",
        "        loss = tf.reduce_mean((q_sa - y) ** 2)\n",
        "\n",
        "        # Loss minimizer\n",
        "        optimizer = tf.train.AdamOptimizer(0.001)\n",
        "        self.train_op = optimizer.minimize(loss, var_list=self.q.trainable_variables)\n",
        "\n",
        "        # Initialize variables\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "        self.sess.run(tf.variables_initializer(optimizer.variables()))\n",
        "        self.update_target_q_weights()  # target Q network 의 파라미터를 Q-newtork 에서 복사\n",
        "\n",
        "    def update_target_q_weights(self):\n",
        "        #################################\n",
        "        # TODO: implement here!\n",
        "        self.q_target.set_weights(self.q.get_weights())\n",
        "        #################################\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = np.max([self.epsilon * self.epsilon_decay, self.epsilon_min])\n",
        "\n",
        "    def epsilon_greedy(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.randint(0, self.num_actions)\n",
        "        else:\n",
        "            q_values = self.sess.run(self.q_values, feed_dict={self.state_ph: np.array([state])})[0]\n",
        "            return np.argmax(q_values)\n",
        "\n",
        "    def train(self):\n",
        "        replay_buffer = ReplayBuffer()\n",
        "        episodes_rewards = []\n",
        "        #for episode in range(5000):\n",
        "        for episode in range(200):\n",
        "            state = self.env.reset()\n",
        "\n",
        "            episode_reward = 0.\n",
        "            for t in range(10000):\n",
        "                # action 선택 및 환경에 실행\n",
        "                action = self.epsilon_greedy(state)\n",
        "                next_state, reward, terminal, info = self.env.step(action)\n",
        "                episode_reward += reward\n",
        "\n",
        "                # Replay buffer 에 (s,a,r,s') 저장\n",
        "                done = terminal and info.get('TimeLimit.truncated') != True\n",
        "                replay_buffer.add(state, action, reward, next_state, done)\n",
        "\n",
        "                # 30 에피소드마다 visualization\n",
        "                '''\n",
        "                if episode % 30 == 0 or episode > 3000:\n",
        "                    q_values = self.sess.run(self.q_values, feed_dict={self.state_ph: np.array([state])})[0]\n",
        "                    print(\"[epi=%4d,t=%4d] state=%4s / action=%s / reward=%7.4f / next_state=%4s / Q[s]=%s\" % (episode, t, state, action, reward, next_state, q_values))\n",
        "                    #env.render()\n",
        "                    time.sleep(0.01)\n",
        "                '''\n",
        "\n",
        "                # Replay buffer에서 {(s,a,r,s')} 미니 배치 샘플 후 gradient descent 한 번 수행\n",
        "                if len(replay_buffer) >= self.batch_size:\n",
        "                    state_batch, action_batch, reward_batch, next_state_batch, done_batch = replay_buffer.sample(self.batch_size)\n",
        "                    self.sess.run(self.train_op, feed_dict={\n",
        "                        self.state_ph: state_batch,\n",
        "                        self.action_ph: action_batch,\n",
        "                        self.reward_ph: reward_batch,\n",
        "                        self.next_state_ph: next_state_batch,\n",
        "                        self.done_ph: done_batch\n",
        "                    })\n",
        "\n",
        "                if terminal:\n",
        "                    break\n",
        "                state = next_state\n",
        "\n",
        "            # 에피소드가 끝날 때마다 epsilon 을 점차 낮춘다.\n",
        "            self.decay_epsilon()\n",
        "\n",
        "            # 에피소드마다 10번마다 target network 의 파라미터를 현재 Q-network 파라미터로 갱신해준다.\n",
        "            if episode % 10 == 0:\n",
        "                self.update_target_q_weights()\n",
        "\n",
        "            episodes_rewards.append(episode_reward)\n",
        "            print('[%4d] Episode reward=%.4f / epsilon=%f' % (episode, episode_reward, self.epsilon))\n",
        "        return episodes_rewards"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45XtAgER7_1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        " \n",
        "# Load environment \n",
        "env_name = 'CartPole-v0'\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# Hyperparameters\n",
        "epsilons = [0.1, 0.5, 0.9]\n",
        "\n",
        "# Train\n",
        "for _epsilon in epsilons:\n",
        "  agent = DQN(env, discount=0.99, epsilon=_epsilon)\n",
        "  episodes_rewards = agent.train()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "dqn_problem.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}